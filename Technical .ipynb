{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi-nk6DAVjmF",
        "outputId": "421432a4-31e9-4a68-c78a-17f1b59a3fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.92\n",
            "Non-Demented\n"
          ]
        }
      ],
      "source": [
        "#forest classifier\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/oasis_cross-sectional-5708aa0a98d82080.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Drop irrelevant columns\n",
        "df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
        "\n",
        "# Handle missing values (fill with median for numerical columns)\n",
        "numeric_cols = [\"Age\", \"Educ\", \"SES\", \"MMSE\", \"CDR\", \"eTIV\", \"nWBV\", \"ASF\", \"Delay\"]\n",
        "df[numeric_cols] = df[numeric_cols].apply(lambda x: x.fillna(x.median()))\n",
        "\n",
        "# Encode categorical variables\n",
        "label_enc = LabelEncoder()\n",
        "df[\"M/F\"] = label_enc.fit_transform(df[\"M/F\"])\n",
        "df[\"Hand\"] = label_enc.fit_transform(df[\"Hand\"])\n",
        "\n",
        "# Define target variable (Dementia classification based on CDR)\n",
        "df[\"Group\"] = (df[\"CDR\"] > 0).astype(int)  # 1 for Demented, 0 for Non-Demented\n",
        "df.drop(columns=[\"CDR\"], inplace=True)  # Remove CDR as it's now encoded in Group\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "numeric_cols.remove(\"CDR\")  # Since we dropped it\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Split data into train and test sets\n",
        "X = df.drop(columns=[\"Group\"])\n",
        "y = df[\"Group\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Function to make a prediction based on user input\n",
        "def predict_dementia(*args):\n",
        "    input_data = pd.DataFrame([args], columns=X_train.columns)  # Use correct column names\n",
        "    input_data[numeric_cols] = scaler.transform(input_data[numeric_cols])  # Scale input data\n",
        "    prediction = model.predict(input_data)[0]\n",
        "    return \"Demented\" if prediction == 1 else \"Non-Demented\"\n",
        "\n",
        "# Example usage\n",
        "print(predict_dementia(75, 12, 2, 28, 1500, 0.7, 1.2, 1, 1, 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6A4vtEiGawIE",
        "outputId": "5cc00c13-c441-4409-eafb-7ea3dd0fddea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unable to create process using 'C:\\Users\\Rohan Dedhia\\anaconda3\\python.exe \"C:\\Users\\Rohan Dedhia\\anaconda3\\Scripts\\pip-script.py\" install torch torchvision torchaudio'\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRhHjJoxRozl",
        "outputId": "d5dca90f-8919-4127-c589-bee2d377f391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels after encoding: [0 1 2 3]\n",
            "Epoch 1/20, Loss: 0.9893, Train Acc: 0.65, Val Acc: 0.78\n",
            "Epoch 2/20, Loss: 0.6564, Train Acc: 0.82, Val Acc: 0.85\n",
            "Epoch 3/20, Loss: 0.3923, Train Acc: 0.83, Val Acc: 0.80\n",
            "Epoch 4/20, Loss: 0.3250, Train Acc: 0.84, Val Acc: 0.83\n",
            "Epoch 5/20, Loss: 0.2870, Train Acc: 0.87, Val Acc: 0.83\n",
            "Epoch 6/20, Loss: 0.2802, Train Acc: 0.86, Val Acc: 0.86\n",
            "Epoch 7/20, Loss: 0.2731, Train Acc: 0.87, Val Acc: 0.86\n",
            "Epoch 8/20, Loss: 0.2679, Train Acc: 0.87, Val Acc: 0.80\n",
            "Epoch 9/20, Loss: 0.2584, Train Acc: 0.88, Val Acc: 0.83\n",
            "Epoch 10/20, Loss: 0.2549, Train Acc: 0.88, Val Acc: 0.83\n",
            "Epoch 11/20, Loss: 0.2401, Train Acc: 0.89, Val Acc: 0.83\n",
            "Epoch 12/20, Loss: 0.2337, Train Acc: 0.88, Val Acc: 0.78\n",
            "Epoch 13/20, Loss: 0.2244, Train Acc: 0.90, Val Acc: 0.82\n",
            "Epoch 14/20, Loss: 0.2281, Train Acc: 0.89, Val Acc: 0.82\n",
            "Epoch 15/20, Loss: 0.2106, Train Acc: 0.90, Val Acc: 0.85\n",
            "Epoch 16/20, Loss: 0.2113, Train Acc: 0.90, Val Acc: 0.85\n",
            "Epoch 17/20, Loss: 0.2109, Train Acc: 0.90, Val Acc: 0.82\n",
            "Epoch 18/20, Loss: 0.1933, Train Acc: 0.92, Val Acc: 0.80\n",
            "Epoch 19/20, Loss: 0.1859, Train Acc: 0.92, Val Acc: 0.80\n",
            "Epoch 20/20, Loss: 0.1879, Train Acc: 0.92, Val Acc: 0.80\n",
            "Test Accuracy: 0.83\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/oasis_cross-sectional-5708aa0a98d82080.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Drop irrelevant columns\n",
        "df.drop(columns=[\"ID\", \"Delay\"], inplace=True, errors='ignore')\n",
        "\n",
        "# Handle missing values\n",
        "df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "df.fillna(df.mode().iloc[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_enc = LabelEncoder()\n",
        "df[\"M/F\"] = label_enc.fit_transform(df[\"M/F\"])\n",
        "df[\"Hand\"] = label_enc.fit_transform(df[\"Hand\"])\n",
        "\n",
        "# Encode target variable \"CDR\"\n",
        "y = df[\"CDR\"]\n",
        "y = label_enc.fit_transform(y)  # Ensures labels start from 0\n",
        "\n",
        "# Check unique labels after encoding\n",
        "print(\"Unique labels after encoding:\", np.unique(y))\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "numeric_cols = [\"Age\", \"Educ\", \"SES\", \"MMSE\", \"eTIV\", \"nWBV\", \"ASF\"]\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "X = df.drop(columns=[\"CDR\"])\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create PyTorch datasets and loaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define Attention Layer\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.attention_weights = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_scores = torch.softmax(self.attention_weights(x), dim=1)\n",
        "        return x * attn_scores\n",
        "\n",
        "# Define ResNet-based Model with Attention\n",
        "class AttentionResNet(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(AttentionResNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.attn1 = AttentionLayer(128)\n",
        "        self.res1 = nn.Linear(128, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.attn2 = AttentionLayer(64)\n",
        "        self.res2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.attn1(x) + self.res1(x)  # Residual connection\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.attn2(x) + self.res2(x)  # Residual connection\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = len(np.unique(y))  # Ensure correct number of classes\n",
        "model = AttentionResNet(input_dim, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with validation\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "        train_acc = correct / total\n",
        "        val_acc = evaluate_model(model, val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}, Val Acc: {val_acc:.2f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in data_loader:\n",
        "            outputs = model(batch_X)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# Train and evaluate\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer)\n",
        "\n",
        "# Final Test Accuracy\n",
        "test_accuracy = evaluate_model(model, test_loader)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPHfB43da3V7",
        "outputId": "c8bb84a5-f603-4050-fb12-7c925fd5a75e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkuse3xw9Zyb",
        "outputId": "9e12590d-01f4-4069-938d-bc9ff3d2a10f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.5901\n",
            "Epoch 2/20, Loss: 0.4095\n",
            "Epoch 3/20, Loss: 0.3431\n",
            "Epoch 4/20, Loss: 0.2867\n",
            "Epoch 5/20, Loss: 0.2512\n",
            "Epoch 6/20, Loss: 0.2178\n",
            "Epoch 7/20, Loss: 0.2080\n",
            "Epoch 8/20, Loss: 0.1623\n",
            "Epoch 9/20, Loss: 0.1492\n",
            "Epoch 10/20, Loss: 0.1261\n",
            "Epoch 11/20, Loss: 0.1054\n",
            "Epoch 12/20, Loss: 0.1028\n",
            "Epoch 13/20, Loss: 0.0826\n",
            "Epoch 14/20, Loss: 0.0861\n",
            "Epoch 15/20, Loss: 0.0719\n",
            "Epoch 16/20, Loss: 0.0521\n",
            "Epoch 17/20, Loss: 0.0439\n",
            "Epoch 18/20, Loss: 0.0392\n",
            "Epoch 19/20, Loss: 0.0363\n",
            "Epoch 20/20, Loss: 0.0416\n",
            "Test Accuracy: 0.98\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/oasis_cross-sectional-5708aa0a98d82080.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Rename columns to match expected names\n",
        "df.rename(columns={\"Educ\": \"EDUC\"}, inplace=True)\n",
        "\n",
        "# Drop irrelevant columns\n",
        "df.drop(columns=[\"ID\", \"Delay\"], inplace=True, errors='ignore')\n",
        "\n",
        "# Handle missing values\n",
        "df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "df.fillna(df.mode().iloc[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_enc = LabelEncoder()\n",
        "df[\"M/F\"] = label_enc.fit_transform(df[\"M/F\"])\n",
        "df[\"Hand\"] = label_enc.fit_transform(df[\"Hand\"])\n",
        "\n",
        "# Define a new target variable (binary classification example)\n",
        "df[\"Target\"] = (df[\"CDR\"] > 0).astype(int)  # Example: Classifying cognitive impairment\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "numeric_cols = [\"Age\", \"EDUC\", \"SES\", \"MMSE\", \"CDR\", \"eTIV\", \"nWBV\", \"ASF\"]\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Reshape data for CNN (adding a temporal axis for concatenation)\n",
        "X = df.drop(columns=[\"Target\"]).values\n",
        "y = df[\"Target\"].values\n",
        "\n",
        "# Convert to 3D tensor format for CNN input (batch_size, channels, sequence_length)\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])  # 1 channel since data is not an image\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define CNN Model with Concatenated Frames\n",
        "class CNNWithConcatenation(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(CNNWithConcatenation, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Pool to single value per channel\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.conv1(x)))\n",
        "        x = torch.relu(self.bn2(self.conv2(x)))\n",
        "        x = torch.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.global_pool(x)  # Reduce dimensions\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_dim = X_train.shape[2]  # Number of features\n",
        "num_classes = len(np.unique(y))\n",
        "model = CNNWithConcatenation(input_dim, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer)\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs = model(batch_X)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1XsTKPF_MS8",
        "outputId": "16cecb18-6ac3-4138-f028-e1afbdd97d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique CDR values: [0.  0.5 1.  2. ]\n",
            "Binned CDR values: [0, 1, 2]\n",
            "Categories (3, int64): [0 < 1 < 2]\n",
            "Epoch 1, Loss: 1.0824\n",
            "Epoch 2, Loss: 0.9516\n",
            "Epoch 3, Loss: 0.8569\n",
            "Epoch 4, Loss: 0.7770\n",
            "Epoch 5, Loss: 0.7343\n",
            "Epoch 6, Loss: 0.6633\n",
            "Epoch 7, Loss: 0.6306\n",
            "Epoch 8, Loss: 0.5692\n",
            "Epoch 9, Loss: 0.5782\n",
            "Epoch 10, Loss: 0.5493\n",
            "Epoch 11, Loss: 0.5147\n",
            "Epoch 12, Loss: 0.5126\n",
            "Epoch 13, Loss: 0.4730\n",
            "Epoch 14, Loss: 0.4443\n",
            "Epoch 15, Loss: 0.4299\n",
            "Epoch 16, Loss: 0.4334\n",
            "Epoch 17, Loss: 0.3981\n",
            "Epoch 18, Loss: 0.4128\n",
            "Epoch 19, Loss: 0.4018\n",
            "Epoch 20, Loss: 0.3622\n",
            "Epoch 21, Loss: 0.3590\n",
            "Epoch 22, Loss: 0.3731\n",
            "Epoch 23, Loss: 0.3461\n",
            "Epoch 24, Loss: 0.3327\n",
            "Epoch 25, Loss: 0.3537\n",
            "Epoch 26, Loss: 0.3482\n",
            "Epoch 27, Loss: 0.3199\n",
            "Epoch 28, Loss: 0.3322\n",
            "Epoch 29, Loss: 0.3222\n",
            "Epoch 30, Loss: 0.2950\n",
            "Epoch 31, Loss: 0.2979\n",
            "Epoch 32, Loss: 0.2900\n",
            "Epoch 33, Loss: 0.2844\n",
            "Epoch 34, Loss: 0.2959\n",
            "Epoch 35, Loss: 0.2869\n",
            "Epoch 36, Loss: 0.2902\n",
            "Early stopping triggered\n",
            "Ensemble Model Accuracy: 0.92\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"/content/oasis_cross-sectional-5708aa0a98d82080.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Drop irrelevant columns\n",
        "df.drop(columns=[\"ID\"], inplace=True, errors='ignore')\n",
        "\n",
        "# Handle missing values\n",
        "df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "df.fillna(df.mode().iloc[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_enc = LabelEncoder()\n",
        "for col in [\"M/F\", \"Hand\"]:\n",
        "    if col in df.columns:\n",
        "        df[col] = label_enc.fit_transform(df[col].astype(str))\n",
        "\n",
        "# Select target variable\n",
        "target_col = \"CDR\" if \"CDR\" in df.columns else None\n",
        "if not target_col:\n",
        "    raise ValueError(\"CDR column not found. Check dataset.\")\n",
        "\n",
        "# Inspect unique CDR values\n",
        "print(\"Unique CDR values:\", df[target_col].unique())\n",
        "\n",
        "# Bin CDR values into categories with improved logic\n",
        "cdr_bins = [0, 0.5, 1.5, 3]\n",
        "cdr_labels = [0, 1, 2]\n",
        "\n",
        "df[target_col] = pd.cut(df[target_col], bins=cdr_bins, labels=cdr_labels, include_lowest=True)\n",
        "print(\"Binned CDR values:\", df[target_col].unique())\n",
        "\n",
        "# Drop NaNs after binning\n",
        "df.dropna(subset=[target_col], inplace=True)\n",
        "\n",
        "# Convert categories to integers\n",
        "df[target_col] = df[target_col].astype(int)\n",
        "\n",
        "# Handle class imbalance\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(df[target_col]), y=df[target_col])\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Split features into categories\n",
        "numeric_cols = [\"Age\", \"Educ\", \"SES\", \"MMSE\", \"eTIV\", \"nWBV\", \"ASF\"]\n",
        "categorical_cols = [col for col in df.columns if col not in numeric_cols + [target_col]]\n",
        "\n",
        "# Scale numerical features\n",
        "df[numeric_cols] = StandardScaler().fit_transform(df[numeric_cols])\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_num = df[numeric_cols]\n",
        "X_cat = df[categorical_cols]\n",
        "y = df[target_col]\n",
        "\n",
        "X_num_train, X_num_test, y_train, y_test = train_test_split(X_num, y, test_size=0.2, random_state=42)\n",
        "X_cat_train, X_cat_test, _, _ = train_test_split(X_cat, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to tensors\n",
        "X_num_train_tensor = torch.tensor(X_num_train.values, dtype=torch.float32)\n",
        "X_num_test_tensor = torch.tensor(X_num_test.values, dtype=torch.float32)\n",
        "X_cat_train_tensor = torch.tensor(X_cat_train.values, dtype=torch.float32)\n",
        "X_cat_test_tensor = torch.tensor(X_cat_test.values, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values.astype(int), dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values.astype(int), dtype=torch.long)\n",
        "\n",
        "# Validate class indices\n",
        "num_classes = len(np.unique(y_train))\n",
        "if y_train_tensor.max().item() >= num_classes:\n",
        "    raise ValueError(f\"Invalid target value. Expected classes in range [0, {num_classes - 1}], but found {y_train_tensor.max().item()}.\")\n",
        "\n",
        "# Create PyTorch datasets\n",
        "num_dataset = TensorDataset(X_num_train_tensor, y_train_tensor)\n",
        "num_loader = DataLoader(num_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define models\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize models\n",
        "num_model = SimpleNN(X_num_train.shape[1], num_classes)\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_cat_train, y_train)\n",
        "\n",
        "# Train the neural network with early stopping\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = optim.Adam(num_model.parameters(), lr=0.001)\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(50):\n",
        "    num_model.train()\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in num_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = num_model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(num_loader)\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Ensemble evaluation\n",
        "num_model.eval()\n",
        "with torch.no_grad():\n",
        "    nn_preds = num_model(X_num_test_tensor).numpy()\n",
        "    rf_preds = rf_classifier.predict_proba(X_cat_test)\n",
        "\n",
        "    combined_preds = 0.6 * nn_preds + 0.4 * rf_preds\n",
        "    final_preds = np.argmax(combined_preds, axis=1)\n",
        "    final_accuracy = np.mean(final_preds == y_test.values)\n",
        "\n",
        "    print(f\"Ensemble Model Accuracy: {final_accuracy:.2f}\")\n",
        "\n",
        "# The model now properly aligns tensor sizes and ensures data integrity before training! ðŸš€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xkFSM0RCcnB",
        "outputId": "961d3d37-9d8f-4850-8e46-40efd4135933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1.3764\n",
            "Epoch 2/20, Loss: 1.1893\n",
            "Epoch 3/20, Loss: 0.8211\n",
            "Epoch 4/20, Loss: 0.3621\n",
            "Epoch 5/20, Loss: 0.3101\n",
            "Epoch 6/20, Loss: 0.2937\n",
            "Epoch 7/20, Loss: 0.2783\n",
            "Epoch 8/20, Loss: 0.2641\n",
            "Epoch 9/20, Loss: 0.2661\n",
            "Epoch 10/20, Loss: 0.2631\n",
            "Epoch 11/20, Loss: 0.2599\n",
            "Epoch 12/20, Loss: 0.2627\n",
            "Epoch 13/20, Loss: 0.2587\n",
            "Epoch 14/20, Loss: 0.2604\n",
            "Epoch 15/20, Loss: 0.2610\n",
            "Epoch 16/20, Loss: 0.2618\n",
            "Epoch 17/20, Loss: 0.2588\n",
            "Epoch 18/20, Loss: 0.2614\n",
            "Epoch 19/20, Loss: 0.2577\n",
            "Epoch 20/20, Loss: 0.2612\n",
            "Test Accuracy: 0.92\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/oasis_cross-sectional-5708aa0a98d82080.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df.drop(columns=[\"ID\", \"Hand\"], inplace=True, errors='ignore')\n",
        "\n",
        "# Handle missing values\n",
        "df.fillna(df.median(numeric_only=True), inplace=True)\n",
        "\n",
        "# Encode categorical variable\n",
        "df[\"M/F\"] = LabelEncoder().fit_transform(df[\"M/F\"])\n",
        "\n",
        "# Select features and target variable (CDR)\n",
        "numeric_cols = [\"Age\", \"Educ\", \"SES\", \"MMSE\", \"eTIV\", \"nWBV\", \"ASF\"]\n",
        "target_col = \"CDR\"\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Reshape data for LSTM\n",
        "sequence_length = 3  # Using previous 3 records to predict the next state\n",
        "def create_sequences(data, target, seq_length):\n",
        "    sequences, labels = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        sequences.append(data[i:i + seq_length].values)\n",
        "        labels.append(target[i + seq_length])\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "X, y = create_sequences(df[numeric_cols], df[target_col], sequence_length)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        final_out = lstm_out[:, -1, :]\n",
        "        return self.fc(final_out)\n",
        "\n",
        "# Initialize model\n",
        "input_dim = X_train.shape[2]\n",
        "hidden_dim = 64\n",
        "num_layers = 2\n",
        "num_classes = len(np.unique(y))\n",
        "model = LSTMModel(input_dim, hidden_dim, num_layers, num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "train_model(model, train_loader, criterion, optimizer)\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs = model(batch_X)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += batch_y.size(0)\n",
        "            correct += (predicted == batch_y).sum().item()\n",
        "    accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "evaluate_model(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqpk7xztEEct"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}